---
date: 2014-12-17 19:40:29 
layout: post
title: hadoop之为何要重新编译本地库，即使已经是64位
categories: hadoop
tags:  hadoop
---

Hadoop1.x中的本地库，作用在于生成文件的校验值，如果在JVM中生成，速度过慢，所以使用本地库（C代码或者直接搞定机器）速度非常快

Hadoop2.x中的本地库使用本地的地方更多，所以强烈建议重新编译一下本地库，虽然没有正确的本地库，Hadoop也能运行，但是不好说以后会出现什么问题。



虽然官方网站现在都已经给出64为的本地文件库，但是运行起来仍然会报错，

原因在于，官方编译的64位本地库所在的机器的libc.so对应的文件，比我们的机器中的libc.so对应的文件版本要高，因此，即使是64位的库，也会出错，要么重写编译，要么升级系统中的libc.so

操作系统 centos 5.8    
hadoop版本为cloudera   hadoop-0.20.2-cdh3u3  
集群中设置支持gzip lzo压缩后，在对压缩文件进行读取或者对输入文件压缩的时候要使用到hadoop的本地库，本地库的默认位置在

	$HADOOP_HOME/lib/native/Linux-amd64-64   (64位操作系统)
	$HADOOP_HOME/lib/native/Linux-i386-32   （32位操作系统）
文件夹中的libhadoop.so文件，就是hadoop的本地库。  
如果本地库不存在，或者本地库与当前操作系统的版本不一致的时候，会报下面的错误：

	11/09/20 17:29:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable
增加调试信息设置

	$ export HADOOP_ROOT_LOGGER=DEBUG,console
	$ hadoop fs -text /test/data/origz/access.log.gz
	2012-04-24 15:55:43,269 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
	error libhadoop.so  /lib64/libc.so.6 required (libc 2.6)    /usr/local/hadoop/lib/native/Linux-amd64-64  
说明系统中的glibc的版本和libhadoop.so需要的版本不一致导致  
查看系统的libc版本
  
	# ll /lib64/libc.so.6
	lrwxrwxrwx 1 root root 11 Apr 24 16:49 /lib64/libc.so.6 -> libc-2.5.so
系统中的版本为2.5  
将系统中的glibc升级为2.9  
下载glibc  

	wget  http://ftp.gnu.org/gnu/glibc/glibc-2.9.tar.bz2

下载glibc-linuxthreads

	wget http://ftp.gnu.org/gnu/glibc/glibc-linuxthreads-2.5.tar.bz2
解压
	
	$tar -jxvf glibc-2.9.tar.bz2
	$cd glibc-2.9
	$tar -jxvf ../glibc-linuxthreads-2.5.tar.bz2
	$cd ..
	$export CFLAGS="-g -O2"
	$./glibc-2.9/configure --prefix=/usr --disable-profile --enable-add-ons --with-headers=/usr/include --with-binutils=/usr/bin
	$make
	#make install
安装编译过程中需要注意三点：

1.要将glibc-linuxthreads解压到glibc目录下。  
2.不能在glibc当前目录下运行configure。  
3.加上优化开关，export CFLAGS="-g -O2"，否则会出现错误 


安装完后，可以查看ls -l /lib/libc.so.6已升级

	lrwxrwxrwx 1 root root 11 Apr 24 16:49 /lib64/libc.so.6 -> libc-2.9.so
测试本地库是否升级
	
	$ export HADOOP_ROOT_LOGGER=DEBUG,console
	$ hadoop fs -text /test/data/origz/access.log.gz
	12/04/25 08:54:47 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 6bb1b7f8b9044d8df9b4d2b6641db7658aab3cf8]
	
	12/04/25 08:54:47 DEBUG util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...
	
	12/04/25 08:54:47 INFO util.NativeCodeLoader: Loaded the native-hadoop library
	12/04/25 08:54:47 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
	
	12/04/25 08:54:47 DEBUG fs.FSInputChecker: DFSClient readChunk got seqno 0 offsetInBlock 0 lastPacketInBlock false packetLen 132100

可以看到将glibc升级后不再报错，已经成功加载本地库
